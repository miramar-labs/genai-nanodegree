# 1. Introduction to Generative AI Fundamentals
## 1.1 Introduction
## 1.2 Meet Your Instructor
## 1.3 What is Generative AI 
## 1.4 Applications of Generative AI
## 1.5 AI and ML Timeline
## 1.6 Exercise: Familiarize Yourself with a Commercial Large Language Model (LLM)
## 1.7 Exercise Soln
## 1.8 Training Generative AI Models
## 1.9 Generation Algorithms
## 1.10 [Exercise: Generating One Token at a Time](./nb/1.10.ipynb)
## 1.11 [Exercise Soln](./nb/1.11.ipynb)
## 1.12 More Generative AI Architectures
## 1.14 Exercise: Educational Startup Scenario
## 1.15 Exercise Solution: Educational Startup Scenario
## 1.16 Lession Review
# 2. Deep Learning Fundamentals
## 2.1 Lesson Overview
## 2.2 What is a Perceptron
## 2.3 The Multi-Layer Perceptron
## 2.4 Training Deep Neural Networks
## 2.5 [Exercise: Classification of Handwritten Digits Uing an MLP](./nb/2.5.ipynb)
## 2.6 What is PyTorch
## 2.7 PyTorch Tensors
## 2.8 PyTorch Neural Networks
## 2.9 PyTorch Loss Functions
## 2.10 PyTorch Optimizers
## 2.11 PyTorch Datasets and Data Loaders
## 2.13 What is Huggingface
## 2.14 Huggingface Tokenizers
## 2.15 Huggingface Models
## 2.16 Huggingface Datasets
## 2.17 Huggingface Trainers
## 2.18 [Exercise: PyTorch and Huggingface Scavenger Hunt](./nb/2.18.ipynb)
## 2.19 [Exercise Soln](./nb/2.19.ipynb)
## 2.20 Pre-Trained Models and Transfer Learning
## 2.21 [Exercise: Transfer Learning Using MobileNetV3](./nb/2.21.ipynb)
## 2.22 [Exercise Soln](./nb/2.22.ipynb)
## 2.23 Lession Review
# 3. Foundation Models
## 3.1 What is a Foundation Model
## 3.2 Foundation Models vs Traditional Models
## 3.3 Architecture and Scale
## 3.4 [Exercise: Use a Foundation Model to Build a Spam Classifier](./nb/3.4.ipynb)
## 3.5 [Exercise Soln](./nb/3.5.ipynb)
## 3.6 Why Benchmarks Matter
## 3.7 The GLUE Benchmarks
## 3.8 The SuperGLUE Benchmarks
## 3.9 Data Used for Training LLMs
## 3.10 Data Scale and Volume
## 3.11 Biases in Training Data
## 3.12 Exercise: Research the Pre-Training Datasets
## 3.13 Disinformation and Misinformation
## 3.14 Environmental and Human Impacts
## 3.15 Exercise: Analyse the Risks of Using a Foundation Model
## 3.16 Exercise Soln
## 3.17 Lesson Review
# 4. Adapting Foundation Models
## 4.1 What is Adaptation
## 4.2 why We Need to Adapt Foundation Models
## 4.3 Retrieval Augmented Generation
## 4.4 Prompt Design Techniques
## 4.5 Prompt Tuning
## 4.6 One and Few-Shot Prompting
## 4.7 Zero-Shot Prompting
## 4.8 In-Context Learning
## 4.9 Chain-of-Thought Prompting
## 4.10 Exercise: Improve Your Queries Using Prompt Design Techniques
## 4.11 Exercise Soln
## 4.12 Using Probing to Train a Classifier
## 4.13 [Create a BERT Sentiment Classifier](./nb/4.13.ipynb)
## 4.14 [Exercise Soln](./nb/4.14.ipynb)
## 4.15 Fine-Tuning
## 4.16 Parameter-Efficient Fine Tuning (PEFT)
## 4.17 [Exercise: Full Fine-Tuning BERT](./nb/4.17.ipynb)
## 4.18 [Exercise Soln](./nb/4.18.ipynb)
## 4.19 Lesson Review
# 5. PROJECT: Apply Lightweight Fine-Tuning to a Foundation Model
## 5.1 Project Overview
## 5.2 Huggingface PEFT Library
## 5.3 Instructions
## 5.4 Project Soln/Submission (PASSED)
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/miramar-labs/genai-nanodegree/blob/main/Course%202%20-%20Generative%20AI%20Fundamentals/nb/PROJECT-LightweightFineTuning-FINAL.ipynb)


