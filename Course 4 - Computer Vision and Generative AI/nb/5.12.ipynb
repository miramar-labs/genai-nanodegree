{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d74a6c5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Fun With Diffusion Models: Hugging Face Diffuser Demo\n",
    "\n",
    "Now that we have seen how to build a Diffusion Models from scratch, we are comfortable with the basic concepts and the inner working on Diffusion Models. We can then effectively use libraries and tools that allow us to work with Diffusion Models. You can leverage these libraries in order to use Diffusion Models in your own work or for your own personal applications!\n",
    "\n",
    "In this Demo we are going to have fun and explore many different applications of Diffusion Models, to showcase their flexibility and the quality of their generations.\n",
    "\n",
    "One of the best open-source libraries out there is the [diffusers](https://huggingface.co/docs/diffusers/index) library by Hugging Face. It contains many ready-to-use implementations of all the most famous and bleeding-edge diffusion models, as well as tools that make using them (and training them) much easier than doing it from scratch. The library is well-maintained and it adds functionalities and the support for new models continuously.\n",
    "\n",
    "In this demo I am going to show you how to use it to do:\n",
    "\n",
    "- Unconditional Image Generation\n",
    "- Conditional Model Generation: text-to-image and image-to-image\n",
    "- Inpainting\n",
    "- Video generation\n",
    "  \n",
    "**NOTE**: the `diffusers` library introduces the concept of \"pipelines\" instead of just \"models\". This is because in general the inputs from the user might need some pre-processing before they can be consumed by a model. The output of the model might need some post-processing too, before it can be consumed by the user. A pipeline handle all of that in a transparent way, by providing an abstract interface that can be used identically across models, independently of their pre-processing and post-processing steps.\n",
    "\n",
    "### Part 1: the HuggingFace ecosystem and Unconditional Image Generation\n",
    "\n",
    "### Part 2: Text-to-image with the Fastest Diffusion Model available\n",
    "Breaking news: while we were preparing this course, a new model came out, called [Stable Diffusion XL - Turbo](https://stability.ai/news/stability-ai-sdxl-turbo). It takes the Stable Diffusion XL model - a large model that usually takes several diffusion steps (20-50 normally) and hence several seconds to generate an image - and distills it down into a much smaller and faster model. This new model can generate an image with only a few steps (1-4 steps). This is starting to approach the ideal solution to the \"impossible triangle\" that we have seen earlier in the lesson.\n",
    "\n",
    "Thanks to the `diffusers` library, we can incorporate this new model in our demo and we get to play with it already!\n",
    "\n",
    "In order to showcase how easy it is to swap models using the diffusers library, we are also going to play with a different model, [Playground V2](https://blog.playgroundai.com/playground-v2/), which is similar to Stable Diffusion but has a different \"character\". I like this model because it makes it easy to produce artistic results with very little effort.\n",
    "\n",
    "### Part 3: Image-to-Image (sketch to image)\n",
    "Let's see how we can transform a sketch into a photo-realistic image using the diffusers library and the [Kandinsky model](https://github.com/ai-forever/Kandinsky-2), yet another Diffusion Model which in my experiments worked very well for this task.\n",
    "\n",
    "### Part 4: Inpainting, and text-to-video\n",
    "Inpainting is the task of fill in (\"inpaint\") empty areas of an image with new content generated by a model. The model will generate the new content by respecting the cues and the context provided by the rest of the image, to return a beliavable final image where the new content and the old content fit together seemlessly.\n",
    "\n",
    "For example, if we have an image that is damaged we could delete the damaged area and replace it with inpainting. Or, if we want to remove an object from an image, we can delete it and then use inpainting to fill the blank area left by the removed object. We can also go to the extreme and remove the entire background of a subject, and then replace it with something completely different.\n",
    "\n",
    "In this demo we are going to see how to do inpainting with the `diffusers` library.\n",
    "\n",
    "We are also going to go beyond image generation, and run a couple of video diffusion models. These are models that can generate short videos starting from text (\"text-to-video\"), or from an existing image (\"image-to-video\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9deb02",
   "metadata": {},
   "source": [
    "## Additional References\n",
    "\n",
    "[]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91afd38",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
