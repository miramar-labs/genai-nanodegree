{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d74a6c5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "![](./img/img47.png)\n",
    "\n",
    "![](./img/img45.png)\n",
    "\n",
    "![](./img/img46.png)\n",
    "\n",
    "There are a variety of language modeling tasks, including statistical tasks like [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) that do not apply directly to transformer-based large language models. In pre-trained models, the pre-training method has a significant impact on its performance against specific tasks. We highlight three particular model types:\n",
    "\n",
    "- Autoregressive (decoder-only)\n",
    "- Autoencoding (encoder-only)\n",
    "- Sequence-to-sequence (encoder-decoder)\n",
    "  \n",
    "Typically, these models are optimized against particular pre-training tasks. [Kalyan et al.](https://arxiv.org/abs/2108.05542) survey these pre-training tasks in depth, but we primarily concern ourselves with the two most popular pre-training tasks:\n",
    "\n",
    "- Causal language modeling - predicting the next token in the sequence\n",
    "- Masked language modeling - filling in the [MASK] token\n",
    "  \n",
    "With these types of pre-training tasks and models, we can perform several popular language modeling tasks:\n",
    "\n",
    "- Text generation\n",
    "- Question answering (both abstractive and extractive)\n",
    "- Summarization\n",
    "- Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9deb02",
   "metadata": {},
   "source": [
    "## Additional References\n",
    "\n",
    "[]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91afd38",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
