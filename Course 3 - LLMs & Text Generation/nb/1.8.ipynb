{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d74a6c5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "LLMs have tunable parameters that change the way they predict next tokens. We can experiment with them in [OpenAI's playground](https://platform.openai.com/playground/) or [together.ai's playground](https://docs.together.ai/docs/inference-web-interface)\n",
    "\n",
    "- `temperature` is a hyperparameter that controls the randomness of the model’s output.\n",
    "  - It controls the 'flatness' of the distribution curve.\n",
    "\n",
    "    - Low temperature (e.g. 0.0 → 0.3)\n",
    "        - More deterministic, focused, repetitive\n",
    "        - The model picks the most probable next token every time\n",
    "        - Great for:\n",
    "            - Factual tasks\n",
    "            - Summaries\n",
    "            - Code generation\n",
    "\n",
    "    - Medium temperature (e.g. ~0.5 → 0.7)\n",
    "        - Balanced creativity and accuracy\n",
    "        - Useful for:\n",
    "            - Creative writing with some factual grounding\n",
    "            - Brainstorming with reasonable coherence\n",
    "\n",
    "    - High temperature (e.g. 0.8 → 1.5 or more)\n",
    "        - More randomness and variation\n",
    "        - The model explores less likely tokens\n",
    "        - Useful for:\n",
    "            - Creative storytelling\n",
    "            - Generating diverse ideas\n",
    "            - Humor, poetry\n",
    "\n",
    "    At temperature = 0 (greedy decoding), the model becomes deterministic: the same prompt yields the same output every time.\n",
    "\n",
    "- `max-tokens` - controls the width of the `Attention Window` of the LLM\n",
    "  - Attention Window comprises of: \n",
    "    - System Prompt\n",
    "    - Chat History\n",
    "    - User Prompt\n",
    "    - max_tokens \n",
    "  \n",
    "- `top-P` - controls greedy decoding \n",
    "  - aka `top-K`\n",
    "  - top-P truncates distribution curve below some value P\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9deb02",
   "metadata": {},
   "source": [
    "## Additional References\n",
    "\n",
    "Hugging Face has a wonderful [blog](https://huggingface.co/blog/how-to-generate) further explaining these decoding parameters. Also, see documentation details for [OpenAI's API](https://platform.openai.com/docs/api-reference/chat/create) and [Together AI's API](https://docs.together.ai/docs/inference-parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91afd38",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
