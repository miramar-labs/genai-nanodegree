{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d74a6c5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Basic Experimental Setup\n",
    "Whenever I’m experimenting with a new LLM, or trying to better understand the capabilities of a given LLM, I do the following:\n",
    "\n",
    "- Because sampling from the next-token probability distribution is inherently stochastic (aka non-deterministic), I enable greedy decoding (by setting temperature = 0).\n",
    "- I find a task that I already know the answer to, and ideally one that can be programmatically evaluated.\n",
    "- I enable the return of token probabilities (sometimes called logprobs for the logarithm of the probability).\n",
    "Completing the above best ensures that my experimentation steps are more reproducible, and that I can evaluate & (partially) introspect the LLM outputs.\n",
    "\n",
    "### Few Shot Prompting\n",
    "In this demo, while experimenting with few-shot prompting, we experienced the following pro/con behavior.\n",
    "\n",
    "- Pro: Sometimes, it's easier to show examples of desired behavior rather than explain the desired behavior.\n",
    "- Con: However, it may be unclear which portions of the examples generalize and which do not, e.g., the LLM appears to over-attend to the fact that 'acres' had 3 anagrams.\n",
    "\n",
    "### Vicious Feedback Loop\n",
    "We also saw the LLM more likely to continue a repeated pattern (in this case, the pattern ‘being, binge’), with increasingly higher prediction probabilities, once the vicious feedback loop was kicked off. This particular LLM behavior is further explored in the paper [The Curious Case of Neural Text Degeneration](https://arxiv.org/abs/1904.09751), which also introduces the top_p sampling mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9deb02",
   "metadata": {},
   "source": [
    "## Additional References\n",
    "\n",
    "[]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91afd38",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
