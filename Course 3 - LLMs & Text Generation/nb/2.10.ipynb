{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d74a6c5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "[Huggingface](https://huggingface.co/) provide the [transformers](https://huggingface.co/docs/transformers/en/index) python library of tools.\n",
    "Part of `transformers` are a number of `pre-trained Tokenizers`, which can be used in various tasks:\n",
    "- Off the shelf LLM use\n",
    "- Fine-tuning\n",
    "- Training from scratch\n",
    "\n",
    "Note - Tokenizers are not 'trained' in the same sense that a neural net is trained on data - they are just algorithms that tokenize text and are picked for specific LLM's prior to training on data. When people say a tokenizer is “pre-trained,” they usually mean:\n",
    "\n",
    "- ✅ The vocabulary (the set of allowed tokens) has been determined in advance.\n",
    "- ✅ The rules for splitting words into tokens were learned from massive text corpora.\n",
    "- ✅ Once chosen, this token vocabulary is frozen and used consistently for the LLM’s entire life.\n",
    "\n",
    "### How Are Tokenizers “Trained”?\n",
    "\n",
    "Let’s clarify the “training” process:\n",
    "\n",
    "- It’s not neural training but statistical or algorithmic analysis of huge text corpora.\n",
    "\n",
    "- The goal is:\n",
    "\n",
    "  - Minimize vocabulary size.\n",
    "\n",
    "  - Maximize ability to represent diverse texts with fewer tokens.\n",
    "\n",
    "- Example (BPE training):\n",
    "\n",
    "  - Corpus: “low lower lowest”\n",
    "\n",
    "  - Initially splits to characters:\n",
    "\n",
    "        [\"l\", \"o\", \"w\", \" \", \"l\", \"o\", \"w\", \"e\", \"r\", ...]\n",
    "\n",
    "  - Finds frequent pairs like “lo,” “ow,” “low.”\n",
    "\n",
    "  - Merges them into new tokens until the vocabulary is full.\n",
    "\n",
    "Once trained, the tokenizer rules are saved and reused. The LLM’s embeddings map these tokens into vectors.\n",
    "\n",
    "### Why Are They Custom to Each Model?\n",
    "\n",
    "A tokenizer must exactly match the model’s embedding layer:\n",
    "\n",
    "- Token ID #1234 in the vocabulary must correspond to the correct embedding vector in the model.\n",
    "\n",
    "- If you swap tokenizers, the LLM might produce nonsense because token IDs don’t match embeddings.\n",
    "\n",
    "That’s why models like GPT-2, GPT-3, LLaMA, Claude, etc. each come with their own tokenizer.\n",
    "\n",
    "### In Short\n",
    "- ✅ Tokenizers are trained, but mostly with algorithms, not neural networks.\n",
    "- ✅ They’re “pre-trained” to create a stable vocabulary and rules before model training.\n",
    "- ✅ They’re custom-built for each model architecture because embeddings depend on token IDs.\n",
    "\n",
    "\n",
    "### Encoding Text as Tokens with Hugging Face\n",
    "To tokenize text with Hugging Face, instantiate a tokenizer object with the `AutoTokenizer.from_pretrained` method. Pass in the name of the model as a string value.\n",
    "\n",
    "        # 'bert-base-cased' can be replaced with a different model as needed\n",
    "        my_tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "Then you can use the tokenizer object to generate either string tokens or integer ID tokens.\n",
    "\n",
    "To generate string tokens, including special tokens:\n",
    "\n",
    "        tokens = my_tokenizer(raw_text).tokens()\n",
    "\n",
    "To generate integer ID tokens you can use the .encode method on raw text, or the .convert_tokens_to_ids method on string tokens.\n",
    "\n",
    "        # Option for raw text\n",
    "        token_ids = my_tokenizer.encode(raw_text)\n",
    "        # Option for string tokens\n",
    "        token_ids = my_tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "### Decoding Tokens to Text with Hugging Face\n",
    "Integer ID tokens can be converted back to text using the .decode method:\n",
    "\n",
    "        decoded_text = my_tokenizer.decode(token_ids)\n",
    "\n",
    "### Unknown tokens\n",
    "Pretrained tokenizers have a predetermined vocabulary. If a token is not in the tokenizer's vocabulary, it will be lost in the encoding + decoding process. In this example, unknown tokens were replaced with [UNK], but this behavior will vary depending on the tokenizer.\n",
    "\n",
    "### Documentation on Hugging Face Tokenizers\n",
    "[PreTrainedTokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer)\n",
    "[AutoTokenizer](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoTokenizer)\n",
    "\n",
    "[Huggingface Tokenizers Exercise](./2.10e.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9deb02",
   "metadata": {},
   "source": [
    "## Additional References\n",
    "\n",
    "[Karparthy: Deep Dive into LLMs like ChatGPT](https://www.youtube.com/watch?v=7xTGNNLPyMI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91afd38",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
