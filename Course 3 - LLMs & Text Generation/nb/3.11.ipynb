{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d74a6c5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Pre-Training Objectives (aka TASKS)\n",
    "Popular transformer-based models differ in not only architecture but also pre-training objectives.\n",
    "\n",
    "- **Autoregressive (AR)**: Predicting the next token using its own last output\n",
    "- **Denoising autoencoder**: Predicting tokens based on the pretext that the data has been corrupted\n",
    "- **Contrastive**: Aligning different inputs or views of the same input and constructing (positive, negative) pairs\n",
    "Some pre-training objectives are better than others for **self-supervised learning**; this depends on whether the ground truth can be constructed within the data structure, or whether it requires manual annotation.\n",
    "\n",
    "### GPT: Decoder Transformers\n",
    "GPT has an **autoregressive** objective. It assumes that there is some kind of continuity or dependency between a value and its predecessors.\n",
    "\n",
    "![](./img/img34.png)\n",
    "\n",
    "The attention scores for future tokens are set to negative infinity to prevent \"cheating\", and then the model proceeds to pick the highest probability candidate for the next token.\n",
    "\n",
    "![](./img/img33.png)\n",
    "\n",
    "A technique called **\"teacher forcing\"**—that has been in use since the 1980s—can be used to prevent the model from accumulating mistakes and continuing on a vicious feedback loop during training.\n",
    "\n",
    "![](./img/img35.png)\n",
    "\n",
    "### BERT: Encoder Transformers\n",
    "BERT has a **denoising autoencoder** objective. Specifically, it uses **masked language modeling (MLM)**:\n",
    "\n",
    "![](./img/img36.png)\n",
    "\n",
    "#### The Problem\n",
    "Traditional language models read text left-to-right or right-to-left. BERT wanted to read both directions at once (i.e. bidirectionally) to capture richer context.\n",
    "\n",
    "#### How MLM Works\n",
    "- Randomly select ~15% of tokens in the input and replace them as follows:\n",
    "\n",
    "  - 80% of the time: Replace with the [MASK] token.\n",
    "\n",
    "  - 10% of the time: Replace with a random word.\n",
    "\n",
    "  - 10% of the time: Keep the original word unchanged.\n",
    "\n",
    "Example:\n",
    "\n",
    "Original:\n",
    "\n",
    "        the cat sat on the mat\n",
    "\n",
    "Masked:\n",
    "\n",
    "        the [MASK] sat on the mat\n",
    "\n",
    "The model’s task:\n",
    "\n",
    "    Predict which word was masked.\n",
    "\n",
    "So, in this example, it should predict “cat.”\n",
    "\n",
    "BERT also optimizes for **next sentence prediction (NSP)**. This is fairly different from a next token prediction—BERT is not generating the next sentence. It is performing a binary classification of whether or not the second sentence belongs after the first.\n",
    "\n",
    "![](./img/img37.png)\n",
    "\n",
    "\n",
    "This task helps BERT learn relationships between sentences, useful for tasks like question answering and natural language inference.\n",
    "\n",
    "#### How NSP Works\n",
    "Given two sentences A and B:\n",
    "\n",
    "- 50% of the time: B is the actual next sentence following A in the corpus.\n",
    "\n",
    "- 50% of the time: B is a random sentence from the corpus.\n",
    "\n",
    "The model is trained to classify whether B follows A.\n",
    "\n",
    "Example:\n",
    "\n",
    "✅ Positive Example:\n",
    "\n",
    "- A: the cat sat on the mat\n",
    "\n",
    "- B: it then fell asleep\n",
    "\n",
    "❌ Negative Example:\n",
    "\n",
    "- A: the cat sat on the mat\n",
    "\n",
    "- B: apples grow in orchards\n",
    "\n",
    "So the NSP head outputs either:\n",
    "\n",
    "- “IsNext”\n",
    "\n",
    "- “NotNext”\n",
    "\n",
    "\n",
    "### Transfer Learning and Domain Adaptation\n",
    "Transformers are popular models for transfer learning and domain adaptation. They are initially pre-trained with a self-supervised process, then additional data can be used for fine-tuning, prompting, and even retrieval-augmented generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9deb02",
   "metadata": {},
   "source": [
    "## Additional References\n",
    "\n",
    "[Improving Language Understanding by Generative Pre-Training (Radford et al., 2018)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
    "\n",
    "[A Learning Algorithm for Continually Running Fully Recurrent Neural Networks (Williams & Zipser, 1989)](https://direct.mit.edu/neco/article-abstract/1/2/270/5490/A-Learning-Algorithm-for-Continually-Running-Fully)\n",
    "\n",
    "[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al., 2018)](https://arxiv.org/abs/1810.04805)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91afd38",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
