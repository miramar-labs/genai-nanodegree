{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d74a6c5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Run the [demo notebook](./1.12d.ipynb) in the [together.ai playground](https://api.together.xyz/)\n",
    "\n",
    "### Chain of Thought Prompting\n",
    "As we explore in this demo, “Chain of Thought” (COT), prompting effectively amounts to asking the LLM to provide step-by-step reasoning for its answer before providing the final answer to the question.\n",
    "\n",
    "COT was first introduced in the paper [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916), and later simplified in a subsequent paper, [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903), and is still in use today. A variation of the COT method was used in Google's [Gemini technical report](https://arxiv.org/abs/2312.11805)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9deb02",
   "metadata": {},
   "source": [
    "## Additional References\n",
    "\n",
    "[]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91afd38",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
