{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d74a6c5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lesson, we learned how to construct a relevant, quality dataset for fine-tuning large language models. When building these datasets, it is important to:\n",
    "\n",
    "- Identify quality data sources\n",
    "- Attempt to collect data from those sources in a way that is minimally disruptive\n",
    "- Evaluate the quality of the collected data for the task and clean it where appropriate\n",
    "- Store the raw data in a task-appropriate way\n",
    "We discussed different tasks for language models and appropriate architectures for those tasks:\n",
    "\n",
    "- Encoder-only models for extractive QA and document similarity\n",
    "- Decoder-only models for open-ended text generation and summarization\n",
    "- Encoder-Decoder models for translating text\n",
    "Finally, we covered how to take the raw data we've collected and, given the task and model at hand, construct a dataset and fine-tune a model using Huggingface `transformers` and `datasets`. Now you have the knowledge to fine-tune models for your own tasks, whatever they may be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9deb02",
   "metadata": {},
   "source": [
    "## Additional References\n",
    "\n",
    "[]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91afd38",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
