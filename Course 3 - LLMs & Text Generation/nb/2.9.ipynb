{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d74a6c5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Normalization\n",
    "  - Cleans text for consistency, eg: case, punctuation, special chars\n",
    "  - Careful not to clean out relevant context\n",
    "- Pre-Tokenization\n",
    "  - Break text into smaller pieces\n",
    "- Tokenization\n",
    "  - ![](./img/img6.png)\n",
    "  - Common Subword Tokenization Algorithms:\n",
    "    - Byte-pair encoding [BPE](https://en.wikipedia.org/wiki/Byte_pair_encoding)\n",
    "      - GPT-2 and RoBERTa\n",
    "    - [WordPiece](https://blog.research.google/2021/12/a-fast-wordpiece-tokenization-system.html)\n",
    "      - BERT and Electra\n",
    "    - [SentencePiece](https://github.com/google/sentencepiece)\n",
    "      - T5, ALBERT, XLNet\n",
    "- Post-Processing\n",
    "  - Apply additional transformations\n",
    "  - eg: Inserting tags:\n",
    "  - ![](./img/img7.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9deb02",
   "metadata": {},
   "source": [
    "## Additional References\n",
    "\n",
    "[Karpathy: Let's Build the GPT-2 Tokenizer!](https://www.youtube.com/watch?v=zduSFxRajkE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91afd38",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
