{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d74a6c5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Syntax & Semantics\n",
    "    - Some domains may use uncommon or specialized syntax/semantics (ge Law/Medicine)\n",
    "    - Be sure to use trusted sources of expert-qualified data in these cases.\n",
    "\n",
    "### Additional Reading: Addressing Biases in Your Dataset\n",
    "Despite the best efforts of researchers and developers, we find that word embeddings consistently contain meaningful gender, racial, and other social biases.\n",
    "\n",
    "Researchers have found that word embeddings, in general, contain [male-centric biases](https://link.springer.com/article/10.1007/s00146-022-01443-w) -- something that has even been found in [ChatGPT](https://aisnakeoil.substack.com/p/quantifying-chatgpts-gender-bias).\n",
    "\n",
    "In some cases, we can accept the risk of these biases not impacting our use case. Even then, especially if we are using data from the internet, we must examine our dataset for the presence of hateful language.\n",
    "\n",
    "In more sensitive cases, we may want to address the bias within our model by modifying our dataset to use more neutral language with respect to some categories.\n",
    "\n",
    "We note, however, that there is a cost to fairness -- debiased models have been found to be marginally less accurate in general. If fairness in machine learning interests you, the [Fair ML book](https://fairmlbook.org/) by Solon Barocas, Moritz Hardt, Arvind Narayanan is a great resource."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9deb02",
   "metadata": {},
   "source": [
    "## Additional References\n",
    "\n",
    "[]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91afd38",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
