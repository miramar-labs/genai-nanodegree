{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d74a6c5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "ðŸ“‡ Embedding Index\n",
    "- A data structure used to store and search embeddings efficiently\n",
    "\n",
    "- Purpose: Enables fast nearest-neighbor lookup (e.g., for semantic search or retrieval)\n",
    "\n",
    "- Often built using libraries like:\n",
    "\n",
    "    - FAISS (Facebook)\n",
    "\n",
    "    - Annoy (Spotify)\n",
    "\n",
    "    - ScaNN (Google)\n",
    "\n",
    "    - Chroma, Weaviate, Pinecone (vector DBs)\n",
    "  \n",
    "ðŸ”„ Relationship\n",
    "|Component|\tRole|\n",
    "|---------------|-------------------------------------------|\n",
    "|Embedding Model|\tGenerates vector representations|\n",
    "|Embedding Index|\tStores those vectors and supports search|\n",
    "\n",
    "ðŸ”§ Example Workflow\n",
    "- Use an embedding model (e.g., OpenAI or SBERT) to encode documents.\n",
    "\n",
    "- Store those embeddings in an index (e.g., FAISS).\n",
    "\n",
    "- When a query comes in:\n",
    "\n",
    "    - Encode it with the same model\n",
    "\n",
    "    - Search the index for nearest neighbors (semantic matches)\n",
    "\n",
    "### Embeddings for 2022 Events Case Study\n",
    "For our dataset, we will use an OpenAI Embedding model, specifically text-embedding-ada-002. This OpenAI model [produces embeddings with 1,536 dimensions](https://openai.com/blog/new-and-improved-embedding-model). Read more in the [API documentation](https://platform.openai.com/docs/guides/embeddings/embeddings).\n",
    "\n",
    "#### Basic Example\n",
    "A basic example of using this model looks like this:\n",
    "\n",
    "    # Generic example code\n",
    "    openai.Embedding.create(\n",
    "        input=[\"text\", \"input\", \"here\"],\n",
    "        engine=\"name-of-model\"\n",
    "    )\n",
    "\n",
    "#### Embeddings Code for 2022 Events Case Study\n",
    "We can generate embeddings for the 2022 Wikipedia page using a similar process. But in this case, we'll send data in batches of 100 in order to avoid rate-limiting issues.\n",
    "\n",
    "    EMBEDDING_MODEL_NAME = \"text-embedding-ada-002\"\n",
    "    batch_size = 100\n",
    "    embeddings = []\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        # Send text data to OpenAI model to get embeddings\n",
    "        response = openai.Embedding.create(\n",
    "            input=df.iloc[i:i+batch_size][\"text\"].tolist(),\n",
    "            engine=EMBEDDING_MODEL_NAME\n",
    "        )\n",
    "\n",
    "        # Add embeddings to list\n",
    "        embeddings.extend([data[\"embedding\"] for data in response[\"data\"]])\n",
    "\n",
    "    # Add embeddings list to dataframe\n",
    "    df[\"embeddings\"] = embeddings\n",
    "\n",
    "**Reminder: All of this code for the case study is available in a Jupyter Notebook on the Case Study Workspace page**\n",
    "\n",
    "After this step, we will have generated and saved embeddings for all rows of our dataset. This is also known as creating an **embeddings index**.\n",
    "\n",
    "You can practice building your own embedding index on the next page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9deb02",
   "metadata": {},
   "source": [
    "## Additional References\n",
    "\n",
    "[]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91afd38",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
