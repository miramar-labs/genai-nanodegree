{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d74a6c5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "When it comes to building question-answering datasets, there are three important components:\n",
    "\n",
    "- Questions\n",
    "- Answers\n",
    "- Contexts\n",
    "  \n",
    "[The Stanford Question Answering Dataset](https://rajpurkar.github.io/SQuAD-explorer/) (SQuAD) is one of the most well-known and studied question-answering datasets. Others, like [HotpotQA](https://hotpotqa.github.io/) and [SubjQA](https://github.com/megagonlabs/SubjQA) are sourced quite differently but are structured quite similarly. The creation of a question-answering dataset can be highly manual and labor intensive, but that effort is not equally distributed.\n",
    "\n",
    "### Collecting Contexts\n",
    "Much like with causal language modeling datasets, we'll need to collect data - either from the internet or from some internal process. These contexts will form the basis of our question-answering dataset, so we'll need to ensure they are trustworthy, relevant, and high quality.\n",
    "\n",
    "Context collection is also potentially a double-edged sword here, as the more contexts we collect, the more questions and answers we will want to create. The effort here is around dataset curation and cleaning, identifying quality data sources, and possibly writing a scraper to collect and store the data.\n",
    "\n",
    "### Questions and Answers\n",
    "Given a corpus of contexts, we can begin writing questions and answers.\n",
    "\n",
    "In some rare cases, we may have an existing set of questions and answers, so we can attempt to map questions and answers to contexts by checking for the presence of the answer in a given document.\n",
    "\n",
    "However, we often have to have humans read documents, generate questions, and identify answers in context.\n",
    "\n",
    "This can be a very labor-intensive process, and while many labs have leveraged crowdsourcing websites that offer small amounts of compensation to workers who complete tasks, [the ethics of such platforms are widely debated](https://www.theatlantic.com/business/archive/2018/01/amazon-mechanical-turk/551192/). Additionally, for tasks that require domain expertise, non-experts may provide poor evaluations that can impact model quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9deb02",
   "metadata": {},
   "source": [
    "## Additional References\n",
    "\n",
    "[]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91afd38",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
