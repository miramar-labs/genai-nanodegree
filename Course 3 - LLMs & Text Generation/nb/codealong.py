# -*- coding: utf-8 -*-
"""codealong.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d3ffZGAUAtyV_IiedctiplTp0l9YvOCV

# Codealong Notebook

Use this notebook as your "scratch pad" as you go through the course contents. Feel free to copy any example code and tweak it to get a better understanding of how it works!

Use the **+** button or `Insert` menu to add additional code cells as needed.

## Step 1

### Loading the Data with `pandas`
"""

import requests

# Get the Wikipedia page for "2022" since OpenAI's models stop in 2021
params = {
    "action": "query",
    "prop": "extracts",
    "exlimit": 1,
    "titles": "2022",
    "explaintext": 1,
    "formatversion": 2,
    "format": "json"
}
resp = requests.get("https://en.wikipedia.org/w/api.php", params=params)
response_dict = resp.json()
response_dict["query"]["pages"][0]["extract"].split("\n")

import pandas as pd

# Show more columns and set column width
pd.set_option('display.max_columns', None)        # Show all columns
pd.set_option('display.width', None)              # Auto-detect best width
pd.set_option('display.max_colwidth', None)       # Show full content of each cell

# Load page text into a dataframe
df = pd.DataFrame()
df["text"] = response_dict["query"]["pages"][0]["extract"].split("\n")

"""Here we try to clean up this text a bit, removing blank lines, headers and prefixing every line with a date

"""

from dateutil.parser import parse

# Clean up text to remove empty lines and headings
df = df[(df["text"].str.len() > 0) & (~df["text"].str.startswith("=="))]

# In some cases dates are used as headings instead of being part of the
# text sample; adjust so dated text samples start with dates
prefix = ""
for (i, row) in df.iterrows():
    # If the row already has " - ", it already has the needed date prefix
    if " ‚Äì " not in row["text"]:
        try:
            # If the row's text is a date, set it as the new prefix
            parse(row["text"])
            prefix = row["text"]
        except:
            # If the row's text isn't a date, add the prefix
            row["text"] = prefix + " ‚Äì " + row["text"]
df = df[df["text"].str.contains(" ‚Äì ")].reset_index(drop=True)

df

"""### Creating an Embeddings Index with `openai.Embedding`

Will use the [latest openai SDK](https://platform.openai.com/docs/guides/embeddings/embeddings?lang=python) `OpenAI().embeddings.create()` instead of the legacy `openai.Embedding.create()`
"""

import openai
import os
try:
    from google.colab import userdata
    api_key = userdata.get('OPENAI_API_KEY')
    if api_key:
        os.environ["OPENAI_API_KEY"] = api_key
except Exception:
    pass
assert os.getenv("OPENAI_API_KEY"), "OPENAI_API_KEY is not set"

client = openai.OpenAI()

"""First we need to decide if/how we are going to chunk up each row of cleaned text in the dataframe. We might want to generate a single embedding for the entire text blob, or one per chunk.

üîç Rule of Thumb:
If your use case benefits from semantic detail at a finer level (e.g., search, question-answering, or classification on parts of the text), then chunk it.
If you're summarizing or classifying the whole paragraph/document, then embedding the full text (if short enough) is okay.


üß† Embedding Full Paragraph (One Embedding per Row):
‚úÖ Good for:
Classification (e.g., spam/not spam, sentiment, topic) of the whole paragraph

When text is short enough to fit under token limits (e.g., <8000 tokens for OpenAI models like text-embedding-3-large)

‚ùå Problems:
Large paragraphs may get truncated or hit model token limits

Embedding gets "blurred" over multiple topics ‚Üí less useful for similarity search


üß† Chunking Paragraph into Smaller Segments (Multiple Embeddings per Row):
‚úÖ Good for:
Semantic search / retrieval (RAG)

QA over documents (so you can retrieve relevant chunks, not whole documents)

Topic detection in specific sections

Fine-grained similarity comparison

üîß Common Strategies:
Chunk by sentence or paragraph with overlapping context

Use fixed-size sliding windows (e.g., 256 or 512 tokens with 20% overlap)

Add metadata: source_id, chunk_id, etc.

Let's try creating one embedding per sentence of row text
"""

# Example using sentence splitting and OpenAI embeddings
from nltk.tokenize import sent_tokenize
import openai
import nltk
import os

# Set NLTK data directory
#nltk_data_dir = "/usr/local/nltk_data"
nltk_data_dir = "./nltk_data"
os.makedirs(nltk_data_dir, exist_ok=True)
nltk.data.path.append(nltk_data_dir)


# Download the required NLTK resource
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab', download_dir=nltk_data_dir)

chunks = []
for idx, row in df.iterrows():
    sentences = sent_tokenize(row["text"])
    for i, sent in enumerate(sentences):
        chunks.append({"original_id": idx, "chunk_id": i, "text": sent})

chunk_df = pd.DataFrame(chunks)

# Now embed `chunk_df["text"]` in batches

import openai
import pandas as pd
import time

# Assume you already have chunk_df with a 'text' column
# Set your model
EMBEDDING_MODEL = "text-embedding-3-small"

# Batching parameters
BATCH_SIZE = 100  # OpenAI allows up to 2048 inputs per request for `text-embedding-3-*`

# Helper: Call OpenAI and handle retries
def get_embeddings_batch(text_list, model=EMBEDDING_MODEL):
    try:
        response = openai.embeddings.create(input=text_list, model=model)
        return [e.embedding for e in response.data]
    except Exception as e:
        print("Retrying after error:", e)
        time.sleep(2)
        return get_embeddings_batch(text_list, model=model)

# Run batching
all_embeddings = []

for i in range(0, len(chunk_df), BATCH_SIZE):
    batch_texts = chunk_df["text"].iloc[i:i+BATCH_SIZE].tolist()
    print(f"Embedding batch {i//BATCH_SIZE + 1} of {len(chunk_df)//BATCH_SIZE + 1}...")
    batch_embeddings = get_embeddings_batch(batch_texts)
    all_embeddings.extend(batch_embeddings)

# Add to DataFrame
chunk_df["embedding"] = all_embeddings

"""## Step 2

### Finding Relevant Data with Cosine Similarity
"""







"""## Step 3

### Tokenizing with `tiktoken`
"""







"""### Composing a Custom Text Prompt"""







"""## Step 4

### Getting a Custom Q&A Response with `openai.Completion`
"""





