{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d74a6c5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- RNNs suffered some drawbacks:\n",
    "  - sequential processing (not GPU friendly)\n",
    "  - forgetting context (vanishing gradients)\n",
    "- LSTM's were able to manage context better but still sequential\n",
    "- Attention finally solved the sequential processing problem.\n",
    "\n",
    "![](./img/img17.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9deb02",
   "metadata": {},
   "source": [
    "## Additional References\n",
    "\n",
    "[StatQuest: Attention for NNs](https://www.youtube.com/watch?v=PSs6nxngL6k)\n",
    "\n",
    "[Attention & Memory in DL/NLP](https://dennybritz.com/posts/wildml/attention-and-memory-in-deep-learning-and-nlp/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91afd38",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
