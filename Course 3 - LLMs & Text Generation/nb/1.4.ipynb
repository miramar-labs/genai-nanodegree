{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15355ebe",
   "metadata": {},
   "source": [
    "### A Transformer is Born\n",
    "![encoder-decoder](./img/img1.png)\n",
    "In mid-2017, Google published [Attention Is All You Need](https://arxiv.org/abs/1706.03762) and introduced the Transformer model to the world. There are many wonderful works explaining the Transformer model, in particular the [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) (a wonderful introduction) and the [Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/) (with a line-by-line implementation in Python).\n",
    "\n",
    "### Encoder-only Models\n",
    "![encoder-only](./img/img2.png)\n",
    "The first LLM to gain broad adoption was [BERT](https://arxiv.org/abs/1810.04805) (Bidirectional Encoder Representations from Transformers), an encoder-only model. Encoder-only models are most commonly used as base models for subsequent fine-tuning with a distinct objective, e.g. for the inference-time task of binary classification of movie reviews.\n",
    "\n",
    "### Decoder-only Models\n",
    "![decoder-only](./img/img3.png)\n",
    "However, before BERT was released, the first [GPT](https://openai.com/research/language-unsupervised) (Generative Pre-Trained Transformer) model, a decoder-only model, was released by OpenAI. Decoder-only models are most commonly used for the inference-time task of text generation. In distinction to encoder-only models, the Transformer's pre-training objective of next token prediction is very similar to the decoder-only model's inference-time task of text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd01fc3d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Original Transformer architecture consisted of multiple layers of both Encoder and Decoders (T5)\n",
    "- BERT dropped the Decoder\n",
    "- GPT (Generative Models) dropped the Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9557ce",
   "metadata": {},
   "source": [
    "## Additional References\n",
    "\n",
    "[3Blue1Brown: Inside an LLM](https://www.youtube.com/watch?v=wjZofJX0v4M&t=183s&pp=ygUZM2JsdWVvbmVicm93biB0cmFuc2Zvcm1lcg%3D%3D)\n",
    "\n",
    "[3Blue1Brown: Attention in Transformers](https://www.youtube.com/watch?v=eMlx5fFNoYc&pp=ygUZM2JsdWVvbmVicm93biB0cmFuc2Zvcm1lcg%3D%3D)\n",
    "\n",
    "[Karpathy: Build GPT from scratch](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=3s&pp=ygUia2FycGF0aHkgdHJhbnNmb3JtZXJzIGZyb20gc2NyYXRjaNIHCQnDCQGHKiGM7w%3D%3D)\n",
    "\n",
    "[Karpathy: Build GPT-2 from scratch](https://www.youtube.com/watch?v=l8pRSuU81PU&t=3s&pp=ygUia2FycGF0aHkgdHJhbnNmb3JtZXJzIGZyb20gc2NyYXRjaA%3D%3D)\n",
    "\n",
    "[Karpathy: Stanford CS25](https://www.youtube.com/watch?v=XfpMkf4rD6E&t=18s&pp=ygUia2FycGF0aHkgdHJhbnNmb3JtZXJzIGZyb20gc2NyYXRjaA%3D%3D)\n",
    "\n",
    "[StatQuest: Transformer from scratch in PyTorch](https://www.youtube.com/watch?v=C9QSpl5nmrY&t=656s)\n",
    "\n",
    "[StatQuest: Transformers clearly explained](https://www.youtube.com/watch?v=zxQyTK8quyY)\n",
    "\n",
    "[StatQuest: Attention clearly explained](https://www.youtube.com/watch?v=bQ5BoolX9Ag)\n",
    "\n",
    "[StatQuest: Encoder-only Transformers (BERT)](https://www.youtube.com/watch?v=GDN649X_acE&pp=0gcJCcMJAYcqIYzv)\n",
    "\n",
    "[StatQuest: Decoder-only Transformers (GPT)](https://www.youtube.com/watch?v=bQ5BoolX9Ag)\n",
    "\n",
    "[Welch Labs: How Deepseek Rewrote the Transformer](https://www.youtube.com/watch?v=0VLAoVGf_74)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5378d3c3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
