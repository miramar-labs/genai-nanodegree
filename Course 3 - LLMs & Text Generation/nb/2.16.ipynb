{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d74a6c5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### RNNs Process Sequences\n",
    "RNNs process text data as a sequence, one piece at a time, with a dependence on previous inputs and using a \"hidden state\" to pass past information farther down the chain.\n",
    "\n",
    "![](./img/img13.png)\n",
    "\n",
    "### Text Generation with RNNs\n",
    "RNNs can generate new text indefinitely by feeding output as input.\n",
    "\n",
    "![](./img/img14.png)\n",
    "\n",
    "### Disadvantages of RNNs\n",
    "- The vanishing gradient problem is when the RNN has trouble \"remembering\" distant/earlier inputs.\n",
    "- RNNs are sequential in nature, which makes training and usage slow, especially when the data are large.\n",
    "  \n",
    "#### What are Vanishing Gradients?\n",
    "Vanishing gradients happen when, during training of a deep neural network, the gradients (the values used to update the modelâ€™s weights) become very small as they move backward through the layers. When gradients are tiny, the model's earlier layers stop learning effectively because the updates to their weights are almost zero. This makes it hard for the model to improve, leading to slow or stalled learning, especially in deep networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9deb02",
   "metadata": {},
   "source": [
    "## Additional References\n",
    "\n",
    "[]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91afd38",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
