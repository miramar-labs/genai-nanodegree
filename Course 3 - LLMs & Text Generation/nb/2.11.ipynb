{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d74a6c5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Hugging Face Tokenizer Properties\n",
    "The Hugging Face API allows you to use a variety of tokenizers, each with its own properties. In this demo, we compared:\n",
    "\n",
    "- 'bert-base-cased'\n",
    "- 'xlm-roberta-base'\n",
    "- 'google/pegasus-xsum'\n",
    "- 'allenai/longformer-base-4096'\n",
    "\n",
    "### Maximum Length\n",
    "Different tokenizers will handle some text better, such as longer input sequences. The `.model_max_length` property of the tokenizer object will tell you the maximum length the model can handle.\n",
    "\n",
    "If the length of your data exceeds the maximum length of your tokenizer, you may need to chunk the data before tokenizing it. Or you could consider switching to a different tokenizer that has a longer maximum length.\n",
    "\n",
    "### Special Tokens\n",
    "Different tokenizers will have different special tokens defined. They might have tokens representing:\n",
    "\n",
    "- Unknown token\n",
    "- Beginning of sequence token\n",
    "- Separator token\n",
    "- Token used for padding\n",
    "- Classifier token\n",
    "- Token used for masking values\n",
    "  \n",
    "Additionally, there may be multiple subtypes of each special token. For example, some tokenizers have multiple different unknown tokens (e.g. <unk> and <unk_2>).\n",
    "\n",
    "### Hugging Face Tokenizers Takeaways\n",
    "**Different tokenizers can create very different tokens for the same piece of text.** When choosing a tokenizer, consider what properties are important to you, such as the maximum length and the special tokens.\n",
    "\n",
    "If none of the available tokenizers perform the way you need them to, you can also fine-tune a tokenizer to adjust it for your use case.\n",
    "\n",
    "### Documentation on Hugging Face Tokenizers and Models\n",
    "- [PreTrainedTokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer)\n",
    "- [AutoTokenizer](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoTokenizer)\n",
    "- Documentation on some available models:\n",
    "    - [bert-base-cased](https://huggingface.co/docs/transformers/model_doc/bert)\n",
    "    - [xlm-roberta-base](https://huggingface.co/docs/transformers/model_doc/xlm-roberta)\n",
    "    - [google/pegasus-xsum](https://huggingface.co/docs/transformers/model_doc/pegasus)\n",
    "    - [allenai/longformer-base-4096](https://huggingface.co/docs/transformers/model_doc/longformer)\n",
    "\n",
    "[Huggingface Tokenizer Properties Demo](./2.11e.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9deb02",
   "metadata": {},
   "source": [
    "## Additional References\n",
    "\n",
    "[]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91afd38",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
